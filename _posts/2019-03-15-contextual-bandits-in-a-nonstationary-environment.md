---
layout: post
title: Contextual Bandits in a Non-Stationary Environment
---

**[Huazheng Wang](http://www.cs.virginia.edu/~hw7ww/)**<br>
Department of Computer Science<br>
University of Virginia

- **Date**: Wednesday March 20th, 2019
- **Time**: 1:00PM
- **Location**: Rice 242

**Abstract** Multi-armed bandit algorithms have become a reference solution for handling the explore/exploit dilemma in recommender systems, and many other important real-world problems, such as display advertisement. However, such algorithms usually assume a stationary reward distribution, which hardly holds in practice as usersâ€™ preferences are dynamic. In this talk, I will introduce three works on non-stationary contextual bandit algorithms, including 1) detecting possible changes of environment, 2) finding a dynamic ensemble of admissible bandit models and 3) modeling user dependency for collaborative learning. Theoretical analysis and empirical evaluations on real-world recommendation datasets validate the effectiveness of the algorithms.
