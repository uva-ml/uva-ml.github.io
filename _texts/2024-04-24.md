---
layout: post
title: "Inverting Language Models"
author: Jack Morris
homepage: "https://jxmo.io/"
image: "https://jxmo.io/img/me.jpg"
org: "Cornell Tech"
time: 2024-04-24
hour: 12:15 - 13:00 ET
loc: "Rice 540"
zoom: ""
editor: Yen-Ling Kuo
---

**Abstract**
How much information do the outputs of NLP models contain about their inputs? We investigate the problem in two scenarios, recovering text inputs from the outputs of embeddings from sentence embedders and next-token probability outputs from language models. In both cases, our methods are able to fully recover some inputs given just the model output. 

**Bio**
Jack Morris is a third-year PhD student at Cornell Tech. He works on NLP and machine learning with applications to security and privacy, with a focus on text-based dense information retrieval systems.

